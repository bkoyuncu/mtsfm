{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSFM Dataset Processing\n",
    "\n",
    "This notebook processes macroeconomic datasets from Hugging Face and prepares them for use with the MacroTS model.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's install the required packages if they're not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install datasets pandas python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Load environment variables and set up dataset paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from typing import Any, Generator, List\n",
    "from datasets import load_dataset, Features, Sequence, Value, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Use environment variables for paths or set defaults\n",
    "MACRO_ECON_PATH = os.environ.get(\"MACRO_ECON_PATH\", \"./dataset\")\n",
    "CUSTOM_DATA_PATH = os.environ.get(\"CUSTOM_DATA_PATH\", \"./dataset\")\n",
    "\n",
    "# Create directory structure\n",
    "csv_dataset_path = os.path.join(MACRO_ECON_PATH, \"csv_dataset\")\n",
    "train_val_path = os.path.join(CUSTOM_DATA_PATH, \"train_val_split\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(csv_dataset_path, exist_ok=True)\n",
    "os.makedirs(train_val_path, exist_ok=True)\n",
    "\n",
    "print(f\"Using MACRO_ECON_PATH: {MACRO_ECON_PATH}\")\n",
    "print(f\"Using CUSTOM_DATA_PATH: {CUSTOM_DATA_PATH}\")\n",
    "print(f\"CSV dataset directory: {csv_dataset_path}\")\n",
    "print(f\"Training/validation split directory: {train_val_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Country Data\n",
    "\n",
    "First, we'll download data for each country from the MacroEcon Hugging Face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all available country codes\n",
    "try:\n",
    "    print(\"Loading dataset info...\")\n",
    "    dataset_info = load_dataset(\"bkoyuncu/MacroEcon\", \"features\")\n",
    "    country_codes = dataset_info['train'][:]['Country Code']\n",
    "    print(f\"Found {len(country_codes)} country codes\")\n",
    "    \n",
    "    # Display first few country codes\n",
    "    print(f\"Sample country codes: {country_codes[:5]}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset info: {e}\")\n",
    "    country_codes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each country code and download data\n",
    "for country in country_codes:\n",
    "    config_name = f\"{country}_data\"\n",
    "    output_path = os.path.join(csv_dataset_path, f\"{country}_features.csv\")\n",
    "    \n",
    "    # Skip if file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"File already exists for {country}, skipping download\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing country: {country}\")\n",
    "        # Load the dataset for this country\n",
    "        country_dataset = load_dataset(\"bkoyuncu/MacroEcon\", config_name)\n",
    "        \n",
    "        # Convert to DataFrame and save as CSV\n",
    "        country_df = pd.DataFrame(country_dataset['train'])\n",
    "        country_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved {country} data to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset for {country}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define functions to process the data and create datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list_of_lists(list_of_lists):\n",
    "    \"\"\"\n",
    "    Converts a list of lists into a single flattened list.\n",
    "    \"\"\"\n",
    "    return [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "def get_file_name(file_path):\n",
    "    \"\"\"\n",
    "    Extract filename without extension from path\n",
    "    \"\"\"\n",
    "    base = os.path.basename(file_path)\n",
    "    return os.path.splitext(base)[0]\n",
    "\n",
    "def filter_dataframe_by_time_span(df, start_time, end_time):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on the given time-span of start and end time indices.\n",
    "    \"\"\"\n",
    "    # Ensure that the index is in datetime format\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # Filter the DataFrame based on the time-span\n",
    "    filtered_df = df[(df.index >= start_time) & (df.index <= end_time)]\n",
    "    return filtered_df\n",
    "\n",
    "def extract_distinct_features(columns):\n",
    "    \"\"\"\n",
    "    Extract distinct features from column names with pattern recognition\n",
    "    \"\"\"\n",
    "    # Dictionary to store features and their row numbers\n",
    "    feature_rows = defaultdict(set)\n",
    "    \n",
    "    # Extract base feature and row for each column\n",
    "    for col in columns:\n",
    "        # Split the column name by underscore\n",
    "        parts = col.split('_')\n",
    "        \n",
    "        # Take the first two parts as the base feature\n",
    "        if len(parts) >= 2:\n",
    "            base_feature = f\"{parts[0]}_{parts[1]}\"\n",
    "            \n",
    "            # Find the row number\n",
    "            row_match = re.search(r'row_(\\d+)', col)\n",
    "            if row_match:\n",
    "                row_num = int(row_match.group(1))\n",
    "                feature_rows[base_feature].add(row_num)\n",
    "    \n",
    "    # Create the simplified feature list\n",
    "    distinct_features = []\n",
    "    for feature, rows in sorted(feature_rows.items()):\n",
    "        feature_rows_list = [f\"{feature}_row_{row}\" for row in sorted(rows)]\n",
    "        distinct_features.append(feature_rows_list)\n",
    "    \n",
    "    return flatten_list_of_lists(distinct_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivar_example_gen_func(df: pd.DataFrame, feature_names, country_name, df_cov=None, feature_names_cov=None) -> Generator[dict[str, Any], None, None]:\n",
    "    \"\"\"\n",
    "    Generator function for multivariate time series examples\n",
    "    \"\"\"\n",
    "    if df_cov is None:\n",
    "        df_cov = pd.DataFrame()\n",
    "    \n",
    "    if feature_names_cov is None:\n",
    "        feature_names_cov = []\n",
    "        \n",
    "    yield {\n",
    "        \"target\": df.to_numpy().T,  # array of shape (var, time)\n",
    "        \"start\": df.index[0],\n",
    "        \"freq\": pd.infer_freq(df.index),\n",
    "        \"item_id\": \"item_0\",\n",
    "        \"country_name\": country_name,\n",
    "        \"column_names\": df.columns.to_list(),\n",
    "        \"feature_names\": feature_names,\n",
    "        \"feature_names_cov\": feature_names_cov\n",
    "    }\n",
    "\n",
    "def create_hf_dataset_from_df(df: pd.DataFrame, output_dir: str, feature_names: List[str], country_name: str, df_cov=None, feature_names_cov=None):\n",
    "    \"\"\"\n",
    "    Create Hugging Face dataset from pandas DataFrame\n",
    "    \"\"\"\n",
    "    if df_cov is None:\n",
    "        df_cov = pd.DataFrame()\n",
    "    \n",
    "    if feature_names_cov is None:\n",
    "        feature_names_cov = []\n",
    "    \n",
    "    features = Features(\n",
    "        dict(\n",
    "            target=Sequence(\n",
    "                Sequence(Value(\"float32\")), length=len(df.columns)\n",
    "            ),  # multivariate time series are saved as (var, time)\n",
    "            start=Value(\"timestamp[s]\"),\n",
    "            freq=Value(\"string\"),\n",
    "            item_id=Value(\"string\"),\n",
    "            country_name=Value(\"string\"),\n",
    "            column_names=Sequence(Value(\"string\")),\n",
    "            feature_names=Sequence(Value(\"string\")),\n",
    "            feature_names_cov=Sequence(Value(\"string\"))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    hf_dataset = Dataset.from_generator(\n",
    "        lambda: multivar_example_gen_func(\n",
    "            df=df, \n",
    "            feature_names=feature_names, \n",
    "            country_name=country_name, \n",
    "            df_cov=df_cov,\n",
    "            feature_names_cov=feature_names_cov\n",
    "        ), \n",
    "        features=features\n",
    "    )\n",
    "    \n",
    "    hf_dataset.save_to_disk(output_dir)\n",
    "    print(f\"Dataset saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Splits\n",
    "\n",
    "Let's define our time-based training splits and process each country's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training splits by date ranges\n",
    "train_splits = {\n",
    "    1: {'start_time': '1984-01-01', 'end_time': '1994-12-31'},  # 80s-early 90s\n",
    "    2: {'start_time': '1996-01-01', 'end_time': '2004-12-31'},  # late 90s-early 2000s\n",
    "    3: {'start_time': '2006-01-01', 'end_time': '2014-12-31'},  # pre-financial crisis to mid-2010s\n",
    "    4: {'start_time': '2016-01-01', 'end_time': '2022-12-31'}   # recent years\n",
    "}\n",
    "\n",
    "print(\"Defined training splits:\")\n",
    "for split_idx, dates in train_splits.items():\n",
    "    print(f\"Split {split_idx}: {dates['start_time']} to {dates['end_time']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all CSV files in the directory\n",
    "all_csv_files = glob.glob(os.path.join(csv_dataset_path, \"*.csv\"))\n",
    "print(f\"Found {len(all_csv_files)} CSV files to process\")\n",
    "\n",
    "# Display first few files\n",
    "if all_csv_files:\n",
    "    print(f\"Sample files: {[os.path.basename(f) for f in all_csv_files[:3]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each country's data and create training splits\n",
    "for csv_file in all_csv_files:\n",
    "    print(f\"\\nProcessing file: {os.path.basename(csv_file)}\")\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "    \n",
    "    # Extract country name from filename\n",
    "    country_name = get_file_name(csv_file).split('_')[0]\n",
    "    print(f\"Country: {country_name}, Data shape: {df.shape}\")\n",
    "    \n",
    "    # Create each split for this country\n",
    "    for split_idx, dates in train_splits.items():\n",
    "        print(f\"\\nCreating split {split_idx} for {country_name} with date range: {dates}\")\n",
    "        \n",
    "        # Filter data by date range\n",
    "        df_split = filter_dataframe_by_time_span(df, **dates)\n",
    "        \n",
    "        # Skip if no data in this time range\n",
    "        if df_split.empty:\n",
    "            print(f\"No data available for {country_name} in time range {dates}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Split data shape: {df_split.shape}\")\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_distinct_features(df_split.columns)\n",
    "        print(f\"Extracted {len(features)} features\")\n",
    "        \n",
    "        # Create output directory\n",
    "        output_dir = os.path.join(train_val_path, f\"{country_name}_split_{split_idx}\")\n",
    "        \n",
    "        # Create and save dataset\n",
    "        create_hf_dataset_from_df(\n",
    "            df_split, \n",
    "            output_dir=output_dir, \n",
    "            feature_names=features, \n",
    "            country_name=country_name,\n",
    "            df_cov=None, \n",
    "            feature_names_cov=[]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Dataset Creation\n",
    "\n",
    "Let's check that the datasets were created correctly by loading and examining one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load US split 1 as an example\n",
    "us_split_1_path = os.path.join(train_val_path, \"us_split_1\")\n",
    "\n",
    "if os.path.exists(us_split_1_path):\n",
    "    try:\n",
    "        ds_multi = Dataset.load_from_disk(us_split_1_path).with_format(\"numpy\")\n",
    "        print(f\"Dataset loaded successfully with {len(ds_multi)} examples\")\n",
    "        print(f\"Available keys: {list(ds_multi[0].keys())}\")\n",
    "        print(f\"Country: {ds_multi[0]['country_name']}\")\n",
    "        print(f\"Target shape: {ds_multi[0]['target'].shape}\")\n",
    "        print(f\"Number of features: {len(ds_multi[0]['feature_names'])}\")\n",
    "        print(f\"Start date: {ds_multi[0]['start']}\")\n",
    "        print(f\"Frequency: {ds_multi[0]['freq']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {us_split_1_path}\")\n",
    "    # Try to find any dataset that was created\n",
    "    datasets = glob.glob(os.path.join(train_val_path, \"*_split_*\"))\n",
    "    if datasets:\n",
    "        print(f\"Other datasets available: {[os.path.basename(d) for d in datasets[:3]]}\")\n",
    "    else:\n",
    "        print(\"No datasets were created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data Visualization\n",
    "\n",
    "Let's visualize some of the macroeconomic time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to plot some time series from the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Get dataset path - try US or use any available dataset\n",
    "    dataset_paths = glob.glob(os.path.join(train_val_path, \"*_split_*\"))\n",
    "    if not dataset_paths:\n",
    "        print(\"No datasets available for visualization\")\n",
    "    else:\n",
    "        dataset_path = dataset_paths[0]\n",
    "        country = os.path.basename(dataset_path).split('_split_')[0]\n",
    "        print(f\"Visualizing data for {country} from {dataset_path}\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        ds = Dataset.load_from_disk(dataset_path).with_format(\"numpy\")\n",
    "        \n",
    "        # Get the time series data\n",
    "        target = ds[0]['target']  # Shape: (variables, time)\n",
    "        feature_names = ds[0]['feature_names']\n",
    "        \n",
    "        # Create time index\n",
    "        start_date = pd.to_datetime(ds[0]['start'])\n",
    "        freq = ds[0]['freq']\n",
    "        time_index = pd.date_range(start=start_date, periods=target.shape[1], freq=freq)\n",
    "        \n",
    "        # Plot a few key indicators\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Look for GDP, inflation, and unemployment features\n",
    "        key_indicators = []\n",
    "        for i, name in enumerate(feature_names):\n",
    "            if any(term in name.lower() for term in ['gdp', 'inflation', 'unemploy', 'interest']):\n",
    "                key_indicators.append((i, name))\n",
    "        \n",
    "        # If we found key indicators, plot them\n",
    "        if key_indicators:\n",
    "            for i, (idx, name) in enumerate(key_indicators[:4]):  # Plot up to 4 indicators\n",
    "                plt.subplot(2, 2, i+1)\n",
    "                plt.plot(time_index, target[idx], label=name)\n",
    "                plt.title(name)\n",
    "                plt.xlabel('Date')\n",
    "                plt.grid(True)\n",
    "        else:\n",
    "            # Otherwise plot the first 4 variables\n",
    "            for i in range(min(4, len(feature_names))):\n",
    "                plt.subplot(2, 2, i+1)\n",
    "                plt.plot(time_index, target[i], label=feature_names[i])\n",
    "                plt.title(feature_names[i])\n",
    "                plt.xlabel('Date')\n",
    "                plt.grid(True)\n",
    "                \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error visualizing data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The dataset processing is now complete. Here's what we accomplished:\n",
    "\n",
    "1. Downloaded country-specific macroeconomic data from Hugging Face\n",
    "2. Saved the raw data as CSV files in `MACRO_ECON_PATH/csv_dataset`\n",
    "3. Split the data into time-based training partitions\n",
    "4. Created Hugging Face datasets in `CUSTOM_DATA_PATH/train_val_split`\n",
    "5. Verified the datasets were created correctly\n",
    "\n",
    "These datasets are now ready to be used for training and evaluating the MacroTS model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
